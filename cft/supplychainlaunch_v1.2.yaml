---
# Copyright 2018 widdix GmbH
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
AWSTemplateFormatVersion: '2010-09-09'
Description: 'VPC: public and private subnets in two availability zones'
Resources:
  VPC:
    Type: 'AWS::EC2::VPC'
    Properties:
      CidrBlock: '10.71.0.0/16'
      EnableDnsSupport: true
      EnableDnsHostnames: true
      InstanceTenancy: default
      Tags:
      - Key: Name
        Value: '10.71.0.0/16'
  InternetGateway:
    Type: 'AWS::EC2::InternetGateway'
    Properties:
      Tags:
      - Key: Name
        Value: '10.71.0.0/16'
  VPCGatewayAttachment:
    Type: 'AWS::EC2::VPCGatewayAttachment'
    Properties:
      VpcId: !Ref VPC
      InternetGatewayId: !Ref InternetGateway
  SubnetAPublic:
    Type: 'AWS::EC2::Subnet'
    Properties:
      AvailabilityZone: !Select [0, !GetAZs '']
      CidrBlock: '10.71.0.0/20'
      MapPublicIpOnLaunch: true
      VpcId: !Ref VPC
      Tags:
      - Key: Name
        Value: 'A public'
      - Key: Reach
        Value: public
  SubnetAPrivate:
    Type: 'AWS::EC2::Subnet'
    Properties:
      AvailabilityZone: !Select [0, !GetAZs '']
      CidrBlock: '10.71.16.0/20'
      VpcId: !Ref VPC
      Tags:
      - Key: Name
        Value: 'A private'
      - Key: Reach
        Value: private
  SubnetBPublic:
    Type: 'AWS::EC2::Subnet'
    Properties:
      AvailabilityZone: !Select [1, !GetAZs '']
      CidrBlock: '10.71.32.0/20'
      MapPublicIpOnLaunch: true
      VpcId: !Ref VPC
      Tags:
      - Key: Name
        Value: 'B public'
      - Key: Reach
        Value: public
  SubnetBPrivate:
    Type: 'AWS::EC2::Subnet'
    Properties:
      AvailabilityZone: !Select [1, !GetAZs '']
      CidrBlock: '10.71.48.0/20'
      VpcId: !Ref VPC
      Tags:
      - Key: Name
        Value: 'B private'
      - Key: Reach
        Value: private
  RouteTablePublic: # should be RouteTableAPublic, but logical id was not changed for backward compatibility
    Type: 'AWS::EC2::RouteTable'
    Properties:
      VpcId: !Ref VPC
      Tags:
      - Key: Name
        Value: 'A Public'
  RouteTablePrivate: # should be RouteTableAPrivate, but logical id was not changed for backward compatibility
    Type: 'AWS::EC2::RouteTable'
    Properties:
      VpcId: !Ref VPC
      Tags:
      - Key: Name
        Value: 'A Private'
  RouteTableBPublic:
    Type: 'AWS::EC2::RouteTable'
    Properties:
      VpcId: !Ref VPC
      Tags:
      - Key: Name
        Value: 'B Public'
  RouteTableBPrivate:
    Type: 'AWS::EC2::RouteTable'
    Properties:
      VpcId: !Ref VPC
      Tags:
      - Key: Name
        Value: 'B Private'
  RouteTableAssociationAPublic:
    Type: 'AWS::EC2::SubnetRouteTableAssociation'
    Properties:
      SubnetId: !Ref SubnetAPublic
      RouteTableId: !Ref RouteTablePublic
  RouteTableAssociationAPrivate:
    Type: 'AWS::EC2::SubnetRouteTableAssociation'
    Properties:
      SubnetId: !Ref SubnetAPrivate
      RouteTableId: !Ref RouteTablePrivate
  RouteTableAssociationBPublic:
    Type: 'AWS::EC2::SubnetRouteTableAssociation'
    Properties:
      SubnetId: !Ref SubnetBPublic
      RouteTableId: !Ref RouteTableBPublic
  RouteTableAssociationBPrivate:
    Type: 'AWS::EC2::SubnetRouteTableAssociation'
    Properties:
      SubnetId: !Ref SubnetBPrivate
      RouteTableId: !Ref RouteTableBPrivate
  RouteTablePublicInternetRoute: # should be RouteTablePublicAInternetRoute, but logical id was not changed for backward compatibility
    Type: 'AWS::EC2::Route'
    DependsOn: VPCGatewayAttachment
    Properties:
      RouteTableId: !Ref RouteTablePublic
      DestinationCidrBlock: '0.0.0.0/0'
      GatewayId: !Ref InternetGateway
  RouteTablePublicBInternetRoute:
    Type: 'AWS::EC2::Route'
    DependsOn: VPCGatewayAttachment
    Properties:
      RouteTableId: !Ref RouteTableBPublic
      DestinationCidrBlock: '0.0.0.0/0'
      GatewayId: !Ref InternetGateway
  NetworkAclPublic:
    Type: 'AWS::EC2::NetworkAcl'
    Properties:
      VpcId: !Ref VPC
      Tags:
      - Key: Name
        Value: Public
  NetworkAclPrivate:
    Type: 'AWS::EC2::NetworkAcl'
    Properties:
      VpcId: !Ref VPC
      Tags:
      - Key: Name
        Value: Private
  SubnetNetworkAclAssociationAPublic:
    Type: 'AWS::EC2::SubnetNetworkAclAssociation'
    Properties:
      SubnetId: !Ref SubnetAPublic
      NetworkAclId: !Ref NetworkAclPublic
  SubnetNetworkAclAssociationAPrivate:
    Type: 'AWS::EC2::SubnetNetworkAclAssociation'
    Properties:
      SubnetId: !Ref SubnetAPrivate
      NetworkAclId: !Ref NetworkAclPrivate
  SubnetNetworkAclAssociationBPublic:
    Type: 'AWS::EC2::SubnetNetworkAclAssociation'
    Properties:
      SubnetId: !Ref SubnetBPublic
      NetworkAclId: !Ref NetworkAclPublic
  SubnetNetworkAclAssociationBPrivate:
    Type: 'AWS::EC2::SubnetNetworkAclAssociation'
    Properties:
      SubnetId: !Ref SubnetBPrivate
      NetworkAclId: !Ref NetworkAclPrivate
  NetworkAclEntryInPublicAllowAll:
    Type: 'AWS::EC2::NetworkAclEntry'
    Properties:
      NetworkAclId: !Ref NetworkAclPublic
      RuleNumber: 99
      Protocol: -1
      RuleAction: allow
      Egress: false
      CidrBlock: '0.0.0.0/0'
  NetworkAclEntryOutPublicAllowAll:
    Type: 'AWS::EC2::NetworkAclEntry'
    Properties:
      NetworkAclId: !Ref NetworkAclPublic
      RuleNumber: 99
      Protocol: -1
      RuleAction: allow
      Egress: true
      CidrBlock: '0.0.0.0/0'
  NetworkAclEntryInPrivateAllowVPC:
    Type: 'AWS::EC2::NetworkAclEntry'
    Properties:
      NetworkAclId: !Ref NetworkAclPrivate
      RuleNumber: 99
      Protocol: -1
      RuleAction: allow
      Egress: false
      CidrBlock: '0.0.0.0/0'
  NetworkAclEntryOutPrivateAllowVPC:
    Type: 'AWS::EC2::NetworkAclEntry'
    Properties:
      NetworkAclId: !Ref NetworkAclPrivate
      RuleNumber: 99
      Protocol: -1
      RuleAction: allow
      Egress: true
      CidrBlock: '0.0.0.0/0'
  NAT:
    DependsOn: VPCGatewayAttachment
    Type: AWS::EC2::NatGateway
    Properties:
      AllocationId:
        Fn::GetAtt:
        - EIP
        - AllocationId
      SubnetId:
        Ref: SubnetAPublic
      Tags:
        - Key: foo
          Value: bar
  S3Endpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
        PolicyDocument:
            Version: 2012-10-17
            Statement:
                - Effect: Allow
                  Principal: '*'
                  Action:
                    - 's3:GetObject'
                  Resource: '*'
        RouteTableIds:
            - !Ref RouteTablePublic
            - !Ref RouteTableBPublic
            - !Ref RouteTablePrivate
            - !Ref RouteTableBPrivate
        ServiceName:  !Join [ "", ["com.amazonaws.",!Ref 'AWS::Region',".s3"] ]
        VpcId: !Ref VPC
  EIP:
    Type: AWS::EC2::EIP
    Properties:
      Domain: !Ref VPC
  RouteANAT:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId:
        Ref: RouteTablePrivate
      DestinationCidrBlock: 0.0.0.0/0
      NatGatewayId:
        Ref: NAT
  RouteBNAT:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId:
        Ref: RouteTableBPrivate
      DestinationCidrBlock: 0.0.0.0/0
      NatGatewayId:
        Ref: NAT
  ScriptBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete
    
  DataBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete  

  RedshiftSG:
    Properties:
      GroupDescription: Redshift security group
      SecurityGroupEgress:
      - CidrIp: 0.0.0.0/0
        IpProtocol: '-1'
      VpcId:
        Ref: VPC
    Type: AWS::EC2::SecurityGroup

  RedshiftQuickSightSG:
    Properties:
      GroupDescription: Redshift security group for QuickSight
      SecurityGroupIngress:
      - CidrIp: 52.15.247.160/27
        IpProtocol: 'tcp'
        FromPort: 0
        ToPort: 65535
      - CidrIp:  52.23.63.224/27
        IpProtocol: 'tcp'
        FromPort: 0
        ToPort: 65535
      - CidrIp: 54.70.204.128/27
        IpProtocol: 'tcp'
        FromPort: 0
        ToPort: 65535
      - CidrIp: 52.210.255.224/27
        IpProtocol: 'tcp'
        FromPort: 0
        ToPort: 65535
      - CidrIp: 35.158.127.192/27
        IpProtocol: 'tcp'
        FromPort: 0
        ToPort: 65535
      VpcId:
        Ref: VPC
    Type: AWS::EC2::SecurityGroup

  InboundRule:
    Properties:
      FromPort: 0
      GroupId:
        Fn::GetAtt:
        - RedshiftSG
        - GroupId
      IpProtocol: '-1'
      SourceSecurityGroupId:
        Fn::GetAtt:
        - RedshiftSG
        - GroupId
      ToPort: 65535
    Type: AWS::EC2::SecurityGroupIngress

  RedshiftSubnetGroup:
    Properties:
      Description: Subnet Group for redshift
      SubnetIds: 
        - !Ref SubnetAPublic
        - !Ref SubnetBPublic
    Type: AWS::Redshift::ClusterSubnetGroup
    
  RedshiftRole:
    Properties:
      AssumeRolePolicyDocument:
        Statement:
        - Action: sts:AssumeRole
          Effect: Allow
          Principal:
            Service: redshift.amazonaws.com
          Sid: ''
        Version: 2012-10-17
    Type: AWS::IAM::Role
    
    
  RedshiftSecret:
    Type: "AWS::SecretsManager::Secret"
    Properties:
      Name: 'supplychainsecret'
      Description: "This is a Secrets Manager secret for Redshift"
      GenerateSecretString:
        SecretStringTemplate: '{"username": "master"}'
        GenerateStringKey: "password"
        PasswordLength: 16
        ExcludeCharacters: '"@/\'
    

  RedshiftCluster:
    Properties:
      ClusterSubnetGroupName:
        Ref: RedshiftSubnetGroup
      ClusterType: "multi-node"
      NodeType: "dc2.large"
      NumberOfNodes: 2
      DBName: supplychain
      Port: 8192
      PubliclyAccessible: true
      IamRoles:
      - Fn::GetAtt:
        - RedshiftRole
        - Arn
      MasterUserPassword: !Join ['', ['{{resolve:secretsmanager:', !Ref RedshiftSecret, ':SecretString:password}}' ]]
      MasterUsername: !Join ['', ['{{resolve:secretsmanager:', !Ref RedshiftSecret, ':SecretString:username}}' ]]
      ClusterParameterGroupName:
        Ref: RedshiftClusterParameterGroup
      VpcSecurityGroupIds:
      - Ref: RedshiftSG
      - Ref: RedshiftQuickSightSG
    Type: AWS::Redshift::Cluster
  
  
  RedshiftClusterParameterGroup: 
    Type: AWS::Redshift::ClusterParameterGroup
    Properties: 
      Description: "Cluster parameter group"
      ParameterGroupFamily: "redshift-1.0"
      Parameters:  
      - ParameterName: "enable_user_activity_logging"
        ParameterValue: "true"  
      - ParameterName: "require_ssl"
        ParameterValue: "true"
      - ParameterName: "wlm_json_configuration"
        ParameterValue: "[ {\"query_concurrency\" : 5,\"query_group\" : [ \"ingest\" ],\"queue_type\" : \"manual\"}, {\"query_concurrency\" : 10,\"queue_type\" : \"manual\"}, {\"short_query_queue\" : true}]"
  
  
  RedshiftPolicy:
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Action:
          - s3:GetBucketLocation
          - s3:GetObject
          - s3:ListBucket
          Effect: Allow
          Resource:
          - arn:aws:s3:::awspsa-sampledata
          - arn:aws:s3:::awspsa-sampledata/supplychain/*
        - Action:
          - glue:CreateDatabase
          - glue:DeleteDatabase
          - glue:GetDatabase
          - glue:GetDatabases
          - glue:UpdateDatabase
          - glue:CreateTable
          - glue:DeleteTable
          - glue:BatchDeleteTable
          - glue:UpdateTable
          - glue:GetTable
          - glue:GetTables
          - glue:BatchCreatePartition
          - glue:CreatePartition
          - glue:DeletePartition
          - glue:BatchDeletePartition
          - glue:UpdatePartition
          - glue:GetPartition
          - glue:GetPartitions
          - glue:BatchGetPartition
          Effect: Allow
          Resource:
          - '*'
        - Action:
          - s3:GetBucketLocation
          - s3:GetObject
          - s3:ListBucket
          - s3:PutObject
          Effect: Allow
          Resource: 
          - !Join ['', ['arn:aws:s3:::', !Ref DataBucket ]]
          - !Join ['', ['arn:aws:s3:::', !Ref DataBucket, '/*']]
        
      PolicyName: redshift_demo_policy
      Roles:
      - Ref: RedshiftRole
    Type: AWS::IAM::Policy    
  
  
  
  GlueExecutionRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - glue.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Path: /
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
  
  GluePolicy:
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Action:
          - s3:GetBucketLocation
          - s3:GetObject
          - s3:ListBucket
          Effect: Allow
          Resource:
          - !Join ['', ['arn:aws:s3:::', !Ref ScriptBucket] ]
          - !Join ['', ['arn:aws:s3:::', !Ref ScriptBucket, '/*'] ]
          - !Join ['', ['arn:aws:s3:::', !Ref DataBucket] ]
          - !Join ['', ['arn:aws:s3:::', !Ref DataBucket, '/*'] ]
        - Action:
          - secretsmanager:*
          Effect: Allow
          Resource: !Ref RedshiftSecret
      PolicyName: glue_demo_policy
      Roles:
      - Ref: GlueExecutionRole
    Type: AWS::IAM::Policy

  
  SetupFunction:
    Type: AWS::Lambda::Function
    Properties:
      Role: !GetAtt 'LambdaExecutionRole.Arn'
      FunctionName: !Join ['-', ['copy_scripts', !Ref 'AWS::StackName']]
      MemorySize: 2048
      Runtime: python3.8.3
      Timeout: 900
      Handler: index.handler
      Code:
        ZipFile: 
          Fn::Sub:
          - |-
           import json
           import boto3
           import os
           import logging
           import cfnresponse

           LOGGER = logging.getLogger()
           LOGGER.setLevel(logging.INFO)

           SCRIPT_SOURCE_BUCKET = 'aws-bigdata-blog'
           SCRIPT_SOURCE_FOLDER = 'artifacts/aws-blog-redshift-based-etl'
           DEST_BUCKET= '${ScriptBucket}'
           UNLOAD_DEST_BUCKET = '${DataBucket}'
           ROLE_ARN = '${Role}'
           CLUSTER_ENDPOINT = '${Endpoint}'
           CLUSTER_ID = CLUSTER_ENDPOINT.split('.')[0]
           ENGINE = 'redshift'
           PORT = '8192'
           SECRET = '${Secret}'

           def handler(event, context):

               # Get CloudFormation-specific parameters, if they exist
               cfn_stack_id = event.get('StackId')
               cfn_request_type = event.get('RequestType')

               
               # Was the function triggered by a CloudFormation resource deletion?
               # If so, delete all objects in dest bucket and return
               if cfn_stack_id and cfn_request_type == 'Delete':
                   s3 = boto3.resource('s3')

                   try:
                       bucket = s3.Bucket(DEST_BUCKET)
                       bucket.objects.delete()
                       bucket = s3.Bucket(UNLOAD_DEST_BUCKET)
                       bucket.objects.delete()
                       message = 'Deleted data'
                   except botocore.exceptions.ClientError as e:
                       # If a client error is thrown, then check that it was a 404 error.
                       # If it was a 404 error, then the bucket does not exist.
                       error_code = int(e.response['Error']['Code'])
                       if error_code == 404:
                           message = 'bucket does not exist.'
                    
                   cfnresponse.send(event, context, cfnresponse.SUCCESS, {
                           'Message': message
                            },
                            context.log_stream_name)

                   return {
                        'statusCode': 200,
                        'body': json.dumps(message)
                    }
                    
               s3 = boto3.client('s3')

               #copy sql files with iam role and bucket replace
               prefix = SCRIPT_SOURCE_FOLDER + '/SQL'
               sqlfiles = s3.list_objects_v2(Bucket=SCRIPT_SOURCE_BUCKET, Prefix = prefix)['Contents']
               for file in sqlfiles[1:]:
                   key = file['Key']
                   obj = s3.get_object(Bucket=SCRIPT_SOURCE_BUCKET, Key=key)['Body'].read().decode('utf-8').replace('rolearn', ROLE_ARN).replace('bucket', UNLOAD_DEST_BUCKET)
                   putkey = 'sql/' + key.split('/')[-1]
                   response = s3.put_object(Bucket=DEST_BUCKET, Key=putkey, Body=obj)
    
               #copy script files directly
               prefix = SCRIPT_SOURCE_FOLDER + '/Python'
               scriptfiles = s3.list_objects_v2(Bucket=SCRIPT_SOURCE_BUCKET, Prefix = prefix)['Contents']
               for file in scriptfiles[1:]:
                   key = file['Key']
                   putkey = 'python/' + key.split('/')[-1]
                   response = s3.copy_object(Bucket=DEST_BUCKET,Key=putkey, CopySource={'Bucket': SCRIPT_SOURCE_BUCKET,'Key':key})

               #copy csv files with iam role and bucket replace
               prefix = SCRIPT_SOURCE_FOLDER + '/SQL'
               sqlfiles = s3.list_objects_v2(Bucket=SCRIPT_SOURCE_BUCKET, Prefix = prefix)['Contents']
               for file in sqlfiles[1:]:
                   key = file['Key']
                   obj = s3.get_object(Bucket=SCRIPT_SOURCE_BUCKET, Key=key)['Body'].read().decode('utf-8').replace('rolearn', ROLE_ARN).replace('bucket', UNLOAD_DEST_BUCKET)
                   putkey = 'sql/' + key.split('/')[-1]
                   response = s3.put_object(Bucket=UNLOAD_DEST_BUCKET, Key=putkey, Body=obj)


               #update Secrets Manager secret with host and port
               sm = boto3.client('secretsmanager')
               sec = json.loads(sm.get_secret_value(SecretId=SECRET)['SecretString'])
               sec['host'] = CLUSTER_ENDPOINT
               sec['port'] = PORT
               sec['engine'] = ENGINE
               sec['dbClusterIdentifier'] = CLUSTER_ID
               newsec = json.dumps(sec)
               response = sm.update_secret(SecretId=SECRET, SecretString=newsec)
                

               # Send a response to CloudFormation pre-signed URL
               cfnresponse.send(event, context, cfnresponse.SUCCESS, {
                   'Message': 'Ingested Data'
                   }, 
                   context.log_stream_name)
                
               return {
                   'statusCode': 200,
                   'body': json.dumps('Copied Files')
               }

          - {
            ScriptBucket : !Ref ScriptBucket,
            DataBucket : !Ref DataBucket,
            Role : !GetAtt RedshiftRole.Arn,
            Endpoint: !GetAtt RedshiftCluster.Endpoint.Address,
            Secret: !Ref RedshiftSecret
            } 

  
  LambdaExecutionRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Path: /
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AWSLambdaExecute
        - arn:aws:iam::aws:policy/AmazonS3FullAccess
        - arn:aws:iam::aws:policy/SecretsManagerReadWrite
  
  EnvSetup:
    Type: 'Custom::EnvSetup'
    DependsOn:
      - LambdaExecutionRole
      - DataBucket
      - ScriptBucket
    Properties:
      ServiceToken: !GetAtt 
        - SetupFunction
        - Arn

  GlueETLJob:
    Type: AWS::Glue::Job
    Properties:
      Command:
        Name: pythonshell
        PythonVersion: 3
        ScriptLocation: 
          Fn::Sub:  
            - "s3://${scriptbucket}/python/rs_query.py"
            - scriptbucket: !Ref ScriptBucket 
      DefaultArguments: 
        "--job-bookmark-option": "job-bookmark-enable" 
        "--extra-py-files": !Join [ "", ["s3://", !Ref ScriptBucket, "/python/redshift_module-0.1-py3.6.egg"] ]
      ExecutionProperty:
         MaxConcurrentRuns: 2
      MaxRetries: 0
      Name: 'rs-query'
      Role: !Ref GlueExecutionRole
  
  GlueRSConnection:
    Type: "AWS::Glue::Connection"
    Properties:
      CatalogId: !Ref 'AWS::AccountId'
      ConnectionInput:
        Description: "JDBC Connection to Redshift"
        PhysicalConnectionRequirements:
          SecurityGroupIdList:
           - !Ref RedshiftSG
          SubnetId: !Ref SubnetAPrivate
        ConnectionType: "JDBC"
        Name: "redshift-conn"
        ConnectionProperties:
          "JDBC_CONNECTION_URL": !Join [ "", ["jdbc:redshift://", !GetAtt RedshiftCluster.Endpoint.Address, ":", !GetAtt RedshiftCluster.Endpoint.Port, "/supplychain?ssl=true"] ]
          "USERNAME": !Join ['', ['{{resolve:secretsmanager:', !Ref RedshiftSecret, ':SecretString:username}}' ]]
          "PASSWORD": !Join ['', ['{{resolve:secretsmanager:', !Ref RedshiftSecret, ':SecretString:password}}' ]]

  GlueJobStateMachine:
    Type: AWS::StepFunctions::StateMachine
    Properties:
      DefinitionString:
        Fn::Sub:
          - |
              {
                "StartAt": "LoadDateJob",
                "States": {
                  "LoadDateJob": {
                    "Type": "Task",
                    "Resource": "arn:aws:states:::glue:startJobRun.sync",
                    "Parameters":{
                      "JobName":"rs-query",
                      "Arguments":{
                        "--db":"supplychain", 
                        "--db_creds":"supplychainsecret",
                        "--bucket": "${ScriptBucket_Sub}",
                        "--file": "sql/copy_into_salesorders.sql",
                        "--parameters": "s3://${DataBucket_Sub}/csv/SalesOrders.csv,${RedshiftRoleArn},${AWS::Region}"
                      }
                    },
                      "Next": "ReportJob",
                      "Catch": [
                        {
                          "ErrorEquals": ["States.TaskFailed"], 
                          "Next": "NotifyFailure",
                          "ResultPath": "$.cause"
                        }
                      ]
                   },
                   
                  "ReportJob": {
                    "Type": "Task",
                    "Resource": "arn:aws:states:::glue:startJobRun.sync",
                    "Parameters":{
                      "JobName":"rs-query",
                      "Arguments":{
                        "--db":"supplychain", 
                        "--db_creds":"supplychainsecret",
                        "--bucket": "${ScriptBucket_Sub}",
                        "--file": "sql/unload_to_datalake.sql",
                        "--parameters": "s3://${DataBucket_Sub}/parquet/supplychain/,${RedshiftRoleArn}"
                      }
                    },
                      "End": true,
                      "Catch": [
                        {
                          "ErrorEquals": ["States.TaskFailed"], 
                          "Next": "NotifyFailure",
                          "ResultPath": "$.cause"
                        }
                      ]
                   },
                    "NotifyFailure": {
                      "Type": "Task", 
                      "Resource": "arn:aws:states:::sns:publish", 
                      "Parameters": {
                        "TopicArn": "${TopicArn}", 
                        "Message.$": "$.cause"
                      },
                      "End": true
                    }
                }
              }
          - { TopicArn: !Ref FailNotificationSNS, ScriptBucket_Sub: !Ref ScriptBucket, DataBucket_Sub: !Ref DataBucket, RedshiftRoleArn: !GetAtt RedshiftRole.Arn  }
 
        
      RoleArn: !GetAtt [ StatesExecutionRole, Arn ]
      
  StatesExecutionRole:
    Type: "AWS::IAM::Role"
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: "Allow"
            Principal:
              Service:
                - !Sub states.${AWS::Region}.amazonaws.com
            Action: "sts:AssumeRole"
      Path: "/"
      Policies:
        -
          PolicyName: statepolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              -
                Effect: "Allow"
                Action:
                  - "glue:StartJobRun"
                  - "glue:GetJobRun"
                  - "glue:GetJobRuns"
                  - "glue:BatchStopJobRun"
                Resource: "*"
              -
                Effect: Allow
                Action:
                  - "sns:publish"
                Resource: !Ref FailNotificationSNS
  
  FailNotificationSNS:  
    Type: AWS::SNS::Topic

Outputs:
  StackName:
    Description: 'Stack name.'
    Value: !Sub '${AWS::StackName}'
  AZs:
    Description: 'AZs'
    Value: 2
  AZA:
    Description: 'AZ of A'
    Value: !Select [0, !GetAZs '']
  AZB:
    Description: 'AZ of B'
    Value: !Select [1, !GetAZs '']
  CidrBlock:
    Description: 'The set of IP addresses for the VPC.'
    Value: !GetAtt 'VPC.CidrBlock'
  VPC:
    Description: 'VPC.'
    Value: !Ref VPC
  SubnetsPublic:
    Description: 'Subnets public.'
    Value: !Join [',', [!Ref SubnetAPublic, !Ref SubnetBPublic]]
  SubnetsPrivate:
    Description: 'Subnets private.'
    Value: !Join [',', [!Ref SubnetAPrivate, !Ref SubnetBPrivate]]
  RouteTablesPrivate:
    Description: 'Route tables private.'
    Value: !Join [',', [!Ref RouteTablePrivate, !Ref RouteTableBPrivate]]
  RouteTablesPublic:
    Description: 'Route tables public.'
    Value: !Join [',', [!Ref RouteTablePublic, !Ref RouteTableBPublic]]
  SubnetAPublic:
    Description: 'Subnet A public.'
    Value: !Ref SubnetAPublic
  RouteTableAPublic:
    Description: 'Route table A public.'
    Value: !Ref RouteTablePublic
  SubnetAPrivate:
    Description: 'Subnet A private.'
    Value: !Ref SubnetAPrivate
  RouteTableAPrivate:
    Description: 'Route table A private.'
    Value: !Ref RouteTablePrivate
  SubnetBPublic:
    Description: 'Subnet B public.'
    Value: !Ref SubnetBPublic
  RouteTableBPublic:
    Description: 'Route table B public.'
    Value: !Ref RouteTableBPublic
  SubnetBPrivate:
    Description: 'Subnet B private.'
    Value: !Ref SubnetBPrivate
  RouteTableBPrivate:
    Description: 'Route table B private.'
    Value: !Ref RouteTableBPrivate
